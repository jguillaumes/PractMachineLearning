---
title: "Practical Machine Learning - Course Project"
author: "Jordi Guillaumes Pons"
date: "22 de setembre de 2015"
output: html_document
---

# Abstract

According to the course project requirements, this document details the steps made to find a suitable predictive model to the physical activity dataset provided and to predict the corresponding outcome for the testing dataset. 

The required R packages and libraries are as follow:

```{r libraries,warning=FALSE,message=FALSE}
require(plyr)
require(dplyr)
require(caret)
require(ggplot2)
require(rpart)
require(ipred)
require(randomForest)
require(survival)
require(gbm)
require(MASS)
require(klaR)
require(knitr)
```

# Exploratory analysis and preprocessing

The training dataset contains 19,622 observations of 160 variables. Most of those variables do not contain information for most of the observations. Our first processing of the training data consists on keeping the variables which are informed for all the observations. That leaves us with 29 variables, excluding the one which contains the outcome. The "testing" data set also provided does not contain the outcome, so it can not be used to test/validate our models. We will split the so called "training" data set into our own training and test data.

```{r cleanse,cache=TRUE}
# Cleaning data - this function selects just the variables with
# data available for all the observations, EXCLUDING the outcome
# values (classe)
cleanData <- function(df) {
  cd <- dplyr::select(df,
           roll_belt,pitch_belt, yaw_belt, total_accel_belt,
           gyros_belt_x, gyros_belt_y, gyros_belt_z,
           accel_belt_x, accel_belt_y, accel_belt_z,
           magnet_belt_x, magnet_belt_y, magnet_belt_z,
           roll_arm, pitch_arm, yaw_arm, total_accel_arm, 
           gyros_arm_x, gyros_arm_y, gyros_arm_z,
           accel_arm_x, accel_arm_y, accel_arm_z,
           magnet_arm_x, magnet_arm_y, magnet_arm_z,
           roll_dumbbell, pitch_dumbbell, yaw_dumbbell)
  cd
}
fulltraining <- read.csv("data/pml-training.csv")
set.seed(1234)
inTrain <- createDataPartition(y=fulltraining$classe,p=0.6,list=FALSE)
clean <- cleanData(fulltraining)
ctraining <- clean[inTrain,]
ctesting <- clean[-inTrain,]
outtrain <- fulltraining$classe[inTrain]
outtest  <- fulltraining$classe[-inTrain]
str(ctraining)
```

The summary suggest that the ranges of the different variables are quite dissimilar, so it is a good idea to center and mean normalize them. We also have still 29 variables, which is a quite big number, so we will reduce the number of effective variables using PCA, selecting the necessary components to explain 95% of the variance.

```{r preprocessing,cache=TRUE}
preProc <- preProcess(ctraining,method=c("center","scale","pca"),thresh=0.95)
preTraining <- predict(preProc, ctraining)
preProc
```

So we have now `r preProc$numComp` variables to consider, which is a good reduction over the original 29. 

# Model selection

The outcome variable we want to predict is a categorical (factor) variable, so the models which give us a continuous value, as Linear Regression, are excluded. We will try to fit models using the methods which are suitable to classification problems:

- Trees (rpart)
- Bagging (treebag)
- Random Forests (rf)
- Boostig with trees (gbm)
- Linear Discriminant Analysis (lda)
- Naive Bayesian Analysis (nb)âˆ«

## Model computation

We will use the train function of the caret package, which takes care of doing the resampling and cross-validation necessary to adjust the models. The default values used by test are usually good enough for that task, so we will not override them. Please take note that running the train functions takes a while, specially for Random Forests and Boosting, so please be patient.

```{r models,cache=TRUE,warning=FALSE}
modRpart    <- train(outtrain ~ .,data=preTraining, method="rpart")
modTreebag  <- train(outtrain ~ .,data=preTraining, method="treebag")
modRForest  <- train(outtrain ~ .,data=preTraining, method="rf")
modGbmBoost <- train(outtrain ~ .,data=preTraining, method="gbm", verbose=FALSE)
modLda      <- train(outtrain ~ .,data=preTraining, method="lda")
modNaive    <- train(outtrain ~ .,data=preTraining, method="nb")                     
```

The accuracies for each of these models are the following:

| Method | Accuracy |
| ------ | -------: |
Trees | `r max(modRpart$results$Accuracy) `
Bagging | `r max(modTreebag$results$Accuracy) `
Random Forests | `r max(modRForest$results$Accuracy) `
Boosting | `r max(modGbmBoost$results$Accuracy)`
Linear Discriminant Analysis | `r max(modLda$results$Accuracy)`
Naive Bayesian | `r max(modNaive$results$Accuracy)`

Perhaps unsurprisely the method which gives better accuracy is **Random Forests**:

```{r printmodel}
modRForest
```

## Model validation

We will now use our test dataset (the one built by ourselves, not the one provided) to verify the model works reasonabily well.

```{r prediction}
preTesting <- predict(preProc, newdata=ctesting)
preds      <- predict(modRForest, newdata=preTesting)
confusionMatrix(data = preds, reference = outtest)
```

As can be seen, we have a good accuracy with our test dataset, so we can conclude this model is good enough to predict the outcomes for the *real* test data.

# Application to the problem test data

We just need to generate the predicted outcomes for the problem test data to finish the assignement.

```{r applying,cache=TRUE}
fullptest <- read.csv("data/pml-testing.csv")
cptest <- cleanData(fullptest)
preCptest <- predict(preProc, newdata=cptest)
answers   <- predict(modRForest, newdata=preCptest)
table(answers)
```
